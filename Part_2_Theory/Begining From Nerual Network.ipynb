{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "1. 我之甘露，彼之毒药\n",
    "2. 从一个简单的预测机开始\n",
    "3. 另一个分类器的例子\n",
    "4. 什么是神经元\n",
    "5. 神经网络是怎么计算的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 我之甘露，彼之毒药\n",
    "\n",
    "其实说起来，我们的计算机，上至超级计算机，下至计算器，从本质上来说并无特别的不同。其实质做的还是加减法，但是它们做这种算术计算是非常非常非常的快。如果我们现在让所有的收银员都不准使用计算机收银而需要心算的话，我估计全世界的超市、杂货店起码得瘫痪一半。但人类也有自己的优势，虽然让我们套公式计算根本比不过计算机，但对于一些更为模糊、不确定的任务，比如根据照片认人这种事，人类自有其优势，而对于计算机来说就显得不是那么容易了，至少要比我们想象的难得多的多。\n",
    "\n",
    "在1秒内从1加到1百万可能对于我们来说是一件不太可能完成的事（除非你提前知道了规律），但是做这种事所用的智能比我们想象的要简单的多的多，这个过程本身并没有太多的技术含量。而我们现今使用电脑、平板、手机这些数码产品去购物、看视频、做工作这件事用到的基础智能本身与1秒内从1加到1百万这件事用到的智能没有太大的区别。\n",
    "\n",
    "如果我们给出一写图片来，人类总能够在这些图片中找到一些信息，比如这张图片是关于一个人的，这张图片是一只猫或者这张图片是一辆车。或者举一个更为极端的例子，我们在看天上云的时候，总会说这朵云像什么像什么，这是我们把现实生活中的经验映射到想象的事物中去了，而让计算机去做这件事情就非常困难，至少要比我们之前想象的要困难的多。\n",
    "\n",
    " ![cat](http://n.sinaimg.cn/news/transform/20171113/puY7-fynship2141885.jpg) \n",
    "\n",
    "\n",
    "我们认为，认出一张图片中有哪些元素需要人类的智能，这些智能是计算机所欠缺的，无论现今计算机的计算能力有多强大，但他们并不拥有人类的智慧。但是计算机除了计算速度非常快之外，还拥有一个人类所没有的优点：只要有电，它们就不知疲倦。\n",
    "\n",
    "相对于改造人类来去在1秒内算出1加到1百万，我们去想办法让计算机认出照片中的一只猫可能实现起来更为容易一些，危害性也更小。而我们的任务，就是找到合适的方法，去让计算机能够实现类似于认出图片中的一只猫这样的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 从一个简单的预测机开始\n",
    "\n",
    "接下来我们会从神经网络开始我们的人工智能之旅，当然在这之前让我们先举一个简单的例子，在这个例子中我们会知道什么是机器学习是怎么回事，我们如何让计算机从数据中学得经验。\n",
    "\n",
    "首先我们需要回忆一下计算机是怎么处理任务的。\n",
    "\n",
    "抽象的看，计算机处理任务一般可以分为三个步骤：\n",
    "输入--计算--输出\n",
    "\n",
    "对于计算机来说，输入输出都是数据，而具体是怎么计算的是需要计算机自己去学习调整的。举个例子，我们现在要实现一个公里与英里换算的功能，而且我们并不知道他们之间的转换公式，我们拥有的只有一堆输入输出的数据。\n",
    "\n",
    "我们所要做的就是从这些数据中找出他们之间的转换关系。\n",
    "\n",
    "让我们开始吧。\n",
    "\n",
    "现在假设我们只知道我们的输入输出之间的关系是线性的。\n",
    "\n",
    "好了，我们知道的已经足够多了，因为知道他们之间的关系是线性的，意味着公里与英里之间的关系是：\n",
    "\n",
    "$$英里数 = 公里数 * c$$\n",
    "\n",
    "而C是一个常数，但是我们不知道这个常数`C`到底是多少。接下来该怎么办，不要忘了我们还有一堆关于公里与英里转换的数据，就像下面这样的：\n",
    "\n",
    "| 序号 | 公里 | 英里 |\n",
    "| -- | -- | -- |\n",
    "| 1 | 0 | 0 |\n",
    "| 2 | 100 | 62.137 |\n",
    "\n",
    "我们究竟该如何找到这个常数`c`呢，找一个随机数其实是一个好主意，比如我们可以让`c = 0.5`，然后代入公式计算一下: $100 * 0.5 = 50$\n",
    "\n",
    "看起来`c = 0.5`至少不是一个坏选择。\n",
    "\n",
    "不过在这之前先看一下50离我们真实输出的值差多少，我们可以简单的用以下公式来计算这个误差的值(**error**)\n",
    "\n",
    "$error = truth - calculated = 62.137 - 50 =12.137$\n",
    "\n",
    "是的，12.137就是常数为0.5的时候，我们计算的输出与真实输出的误差。对于我们来说，这个**error**有什么作用？\n",
    "\n",
    "一个重要的作用就是，**error**可以体现出现在的常数**c**是不是足够好，我们是否需要再调整**c**使得变得更好一点。\n",
    "\n",
    "至少现在看起来我们还有很大的余地去优化这个常数**c**，而且从这个公式来看，如果增加这个常数，我们输出值也会相应的增加，所以我们可以再尝试让`c = 0.6`，再带入公式，还是同样的输入，$100 * 0.6 = 60$ ，再计算一下`error`值，我们发现现在误差变成了`2.137`，效果非常理想!\n",
    "\n",
    "如果我们的标准定的低一些的话，2.137的误差也算是可以接受的，但是我们不能对自己要求放的太低，所以接下来，我们让常数c再加0.1，变成0.7,带入公式后发现，$error = 62.137 - 70 = -7.863$了，我们的误差反而变大了！\n",
    "\n",
    "这说明了我们的步子迈的太大，扯着蛋了，0.6要比0.7效果更好，所以我们能做的就是，退回上一步，步子迈的小一点，再来一次。\n",
    "\n",
    "我们让常数**c**加0.001，变成0.61，再代一次公式，$error = 62.137 - 61 = 1.137$ ，我们的调整很有效。\n",
    "\n",
    "上面这个过程至少可以教给我们一件事，就是我们每次调整的幅度太大，有可能让我们的**error**快速变小，但也很可能过犹不及，而**error**r是一个指导我们调整参数的非常重要的值，如果**error**，可能就意味着我们调整的幅度要大一些，而**error**不是很大的话，那可能调整的幅度就需要小一点了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 另一个分类器的例子\n",
    "\n",
    "之前我们介绍了一个根据输入预测输出的预测机的例子，通过调整内部的参数，利用error来指导我们调整的幅度。接下来我们要介绍一个分类器，先看下面的图：\n",
    "\n",
    "![classify-1](img/classify-1.jpg)\n",
    "\n",
    "从图上我们可以清楚的看到分成了两个组，位于图坐标上半部分的是毛毛虫，它们又细又长，位于坐标轴下半部分的是瓢虫，它们又宽又短。我们可以先回忆一下预测机的例子，我们是通过调整参数c而调整了线性方程的斜率，而现在我们想要把这两组给区分开来，是不是也可以通过这根斜线呢？\n",
    "\n",
    "答案是可以的，如果我们有一根直线把两组分开，斜线的上半部分是一组，斜线的下半部分是另一组，这样的话我们就可以根据这根直线区分的部分来区分是毛毛虫还是瓢虫了。\n",
    "\n",
    "但是现在我们还不知道这根线是怎么样的，如果我们随意画一条直线，有可能结果是这样的：\n",
    "\n",
    "![classify-2](img/classify-2.png)\n",
    "\n",
    "也有可能是这样的：\n",
    "\n",
    "![classify-4](img/classify-4.png)\n",
    "\n",
    "但是我们最好的应该是这样的，一条直线完全把毛毛虫和瓢虫区分开：\n",
    "\n",
    "![classify-3](img/classify-3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 训练一个线性分类器\n",
    "\n",
    "在上面的介绍中我们至少了解到了一件事，完成这个分类器只需要一根合适的直线，接下来我们的任务就是如何找到这根直线。与上一节中预测机的例子的一样，我们依然需要一些真实的数据来帮助我们完成这件事情：\n",
    "\n",
    "| 序号 | 宽度 | 长度 | 种类 |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| 1 | 3.0 | 1.0 | 瓢虫 |\n",
    "| 2 | 1.0 | 3.0 | 毛毛虫 |\n",
    "\n",
    "在上面这张表中是我们所知的真实数据，接下来我们需要利用这些数据来训练出一个分类器，因此这些用来训练的数据我们将它们叫做：**训练集（train set）**。\n",
    "\n",
    "首先让我们把训练集的数据画在坐标轴上，这样看起来会非常直观：\n",
    "\n",
    "![classify-5](img/classify-5.png)\n",
    "\n",
    "我们用红色点代表又细又长的毛毛虫，绿色点代表又肥又短的瓢虫，接下来我们再回想一下我们需要的什么？一根把毛毛虫和瓢虫分开的直线，就像预测机的例子一样，我们可以用一个线性的方程来代表这根直线：\n",
    "\n",
    "$$y=Ax$$\n",
    "\n",
    "其中$y$代表长度，$x$代表宽度，而我们可以通过调节参数$A$来调节直线的斜率。那$A$的初始值呢？随机取值依然是一个理想（或无奈）的选择。\n",
    "\n",
    "首先让$A=0.25$，这样方程就变成了$y=0.25x$，我们可以在坐标轴上画出这条直线，看一下效果如何。\n",
    "\n",
    "![classify-6](img/classify-6.png)\n",
    "\n",
    "我们可以从图上看到，$A=0.25$不是一个好的选择，它完全没有区分开两个不同的群组。所以我们需要增加这根直线的斜率使得它更加陡峭一些，但是我们应该怎么调整它呢？最好是每次调整都的有理有据，这样才能够快速的找到参数$A$的最佳取值。\n",
    "\n",
    "让我们把眼光聚焦在我们的训练集上，看看第一条数据，如果我们把宽度代入公式的话，那么输出的长度应该是：\n",
    "\n",
    "$$y = (0.25)*(3.0) = 0.75$$\n",
    "\n",
    "我们知道在参数**A**取值为0.25的情况下，这个输出这个值太小了，我们预期的输出值应该是1。\n",
    "\n",
    "现在我们又找到一个**误差（error）**，在上一节公里英里转换器的例子中我们利用了这个误差值来调整参数**A**，在我们分类器的例子中同样需要用这个误差来调整我们的参数。\n",
    "\n",
    "在这之前，我们需要先思考一下：$y$的值应该是多少？如果$y$的值是1.0，那么意味着瓢虫的例子正好穿过这条直线。这条直线完美的匹配我们的训练数据，但是它并不是我们所需要的，我们需要的是一条能区分毛毛虫和瓢虫。\n",
    "\n",
    "那么我可以试着找一个点(3.0, 1.1)或者(3.0, 1.2)或者(3.0, 1.3)，当然我们不能选择一个太夸张的值比如10或者100。我们就让输出值$y=1.1$，误差**E**就变成了 $E = 1.1 - 0.75 = 0.35$\n",
    "\n",
    "在这个任务中我们有了一个误差，所以接下来我们要怎样做才能让这个误差帮助我们修正参数呢？如果细究的话，我们其实是希望通过**E**来优化**A**，所以我们其实是需要找出**A**与**E**之间的关系。\n",
    "\n",
    "让我们回到最初的设定的模型：\n",
    "\n",
    "$$y = Ax$$\n",
    "\n",
    "现在我们设我们预期输出为**t**，我们需要微调参数**A**，可以把微调的部分用$\\Delta A$来表，因此我们的公式可以写成：\n",
    "\n",
    "$$t = (A + \\Delta A)x$$\n",
    "\n",
    "再回想一下我们关于误差**E**的定义，所谓的误差就是训练集中真实的输出与我们使用假设参数**A**而得到输出之间的差值，用公式表示的话$E = t - y$。\n",
    "\n",
    "我们再用公式把换算关系捋一捋：\n",
    "\n",
    "$$t - y = (A + \\Delta A)x - Ax$$\n",
    "$$E = t - y = Ax + ( \\Delta A)x - Ax$$\n",
    "$$E = (\\Delta A)x$$\n",
    "\n",
    "这样一来我们就把误差**E**与参数**A**联系起来了。接下来的工作是如何利用误差**E**来调整参数**A**，回到我们上一步工作，把$\\Delta A$替换掉：\n",
    "\n",
    "$$\\Delta A = E / x$$\n",
    "\n",
    "在之前的计算结果中，我们得到的结果是，在x取值为3.0的情况下，得到的误差为0.35。带入公式$\\Delta A = E / x$，$0.35 / 3.0 = 0.1167$，这意味着，我们需要调整现在的参数$A = 0.25$，而调整的值就是0.1167。据此我们可以得到调整后的参数为：$0.25 + 0.1167 = 0.3367$。我们可以用调整后的参数**A**进行计算，就可以得到我们希望得到输出**y**。\n",
    "\n",
    "经过上面的计算，我们已经完成了一个真实数据的训练，接下来让我们进行第二个真实数据的训练。\n",
    "\n",
    "先让我们计算一下在$A = 0.3667$的情况下，当x = 1.0时，$y = 0.3667 * 1.0 = 0.3667$，对比一下真实数据我们可以发现这不是一个好结果。现在我们可以考虑让预期的输出$y = 2.9$，那么我们的误差**E**就变成了(2.9 - 0.3667) = 2.5333，所以我们的参数**A**就变成了0.3667 + 2.5333 = 2.9。现在让我们把刚才的计算通过图形的方式展现出来。\n",
    "\n",
    "有没有看出问题？我们工作的成果好像就是让那根直线贴着两个训练数据兜来兜去！完全没有那种干脆的一刀分开两个不同样本的快感。其实从我们训练的过程中可以看出，如果我们有100个样本，最后这条分割的线会停留在靠近最后一个训练数据的地方，这样的话我们的结果其实就是最后一个训练数据是什么样的。\n",
    "\n",
    "怎么解决呢？其实这涉及到机器学习中一个很重要的原则：我们所有的调整都应该是平稳的。这就意味着我们的模型变化也应该是平稳的。\n",
    "\n",
    "让每次的变化都是平稳的，除了能够避免每次调整幅度太大之外，另外一个最重要的原因在于，我们的真实的训练数据可能有许多的噪声，如果每次都是根据上一个的数据进行调整，那么我们最终训练的结果受噪声影响的可能性就非常大了。\n",
    "\n",
    "让我们回到上一步，但是这一次我们需要让$\\Delta A$变化的更加平稳，因此我们可以再引入一个参数来使得我们的变化更为平缓：\n",
    "\n",
    "$$\\Delta A = L ( E / x )$$\n",
    "\n",
    "这个影响我们变化程度的因素，我们就称为**学习率（learning rate）**，我们在这里用**L**表示。我们可以先让$L = 0.5$，这个取值很简单也很直观，我们每次只需要修改之前的一半。现在我们把所有工作重新来一遍。\n",
    "\n",
    "我们初始的参数$A = 0.25$，我们使用第一个训练数据得到的结果为 $y = 0.25 * 3.0 = 0.75$。对于我们设定的预期输出结果1.1来说，会得到误差为0.35，所以$\\Delta A = L ( E / x ) = 0.5 * 0.35 / 3.0 = 0.0583$，我们需要更新的参数**A**就变成了0.25 + 0.0583 = 0.3083。\n",
    "\n",
    "好了，我们现在把训练数据带入新的公式，得到的结果为：$y = 0.3083 * 3.0 = 0.9250$。现在我们的用来分隔的直线还是在错误的一端，不过这对于我们来说不算一个坏消息，因为我们放缓了调整的脚步，所以这种情况很容易发生。\n",
    "\n",
    "现在让我们代入第二个训练数据，在参数**A**为0.3083的情况下我们可以得到$y = 0.3083 * 1.0 = 0.3083$，我们预期要得到的值为0.9，所以误差为0.9 - 0.3083 = 0.5917。$\\Delta A = L ( E / x ) = 0.5 * 0.5917 / 1.0 = 0.2958$，新的参数**A**调整为0.3083 + 0.2958 = 0.6042。\n",
    "\n",
    "让我们再把这条直线画出来，看看我们的训练成果，我们的分类器完成了！画出了一条漂亮的直线！\n",
    "\n",
    "在这个例子中，当我们引入**learning rate**之后，我们很快的就找到了合适的参数**A**。\n",
    "\n",
    "经过上述的计算，我们找到了一个方法让计算机找到一个有效分类群组！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 什么是神经元\n",
    "\n",
    "人类，或者动物的大脑一直让科学家很困惑，因为即使是像鸽子的大脑也要比我们的计算机复杂的多，而我们的计算机，拥有数不清的计算单元，巨量的存储空间以及不知疲倦的能力。无论怎么想，电子计算机的处理能力都应该比生物大脑要强得多。\n",
    "\n",
    "我们可以仔细想一下这两者之间的差异。传统的电子计算机处理数据冰冷、精确、快速、有效，结果只有两种，正确或不正确。而生物大脑处理速度要慢得多，而且充满不确定。是否正是这些我们看起来的缺陷，才是生物大脑的优势所在呢？\n",
    "\n",
    "首先让我们看一下生物大脑的基本处理单元，**神经元**:\n",
    "\n",
    "![neuron](img/neuron.png)\n",
    "图片来源(http://blog.csdn.net/u013007900/article/details/50066315)\n",
    "\n",
    "神经元虽然形状各异，但是处理的机制大抵相同，信号从树突开始沿着轴突传递到突触，最终传递到下一个神经元。就是通过这种方式，我们的身体能够感受到光，热，能够走，能够跑，能够劳动。那么一个普通人，像千万人一样能跑能跳，能感受光与热，需要多少神经元呢？\n",
    "\n",
    "答案是：1000亿。\n",
    "\n",
    "而像果蝇这么小的生物，就有10万个神经元才能完成飞翔、躲避危险、寻找食物以及其它一系列的任务。而一个线虫只有302个神经元，对于现代计算机来说，这个体量级的数据是微不足道的，但是就是这个数量级的神经元，已经能够完成许多现代计算机所完成不了的任务了。\n",
    "\n",
    "所以呢？生物的秘密到底是什么？为什么看上去脆弱，计算又缓慢的生物大脑可以完成那么多现代计算机都完成不了的任务呢？答案是：不知道。而且这也不是我们需要追寻的，我们所需要了解的是，生物的神经网络给了我们一个启示，对于原先计算机很难执行的任务我们可以借鉴神经网络的一些原理尝试着去解决。在计算机科学家的努力下，他们发现对于图像识别，语音识别等领域，神经网络的模型都取得了非常好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 什么是神经网络\n",
    "\n",
    "所以，让我们回过头来再看下神经网络是怎么运行的。它是接收到了一个信号，处理，输出信号，这个过程其实与我们之前的预测机与分类器的例子有点类似，接受输入的数据，进行处理，输出一个结果。\n",
    "\n",
    "所以呢？是不是意味着我们可以把线性方程拿来当作神经网络用呢？\n",
    "\n",
    "答案是不可以。\n",
    "\n",
    "神经网络并不仅仅是输入一个数据输出一个数据而已，应该说，神经元会接收一个信号，但不会立即触发输出，而是等待输入到一定的阈值才会触发输出。其实神经网络这么实现有其优点，这样就不会因为一些干扰的信号就触发输出，而是等待特定的输入信号足够强。\n",
    "\n",
    "如果有一个函数，接收输入信号并产生输出信号，但在这个过程中还考虑到了一些阈值的因素，我们就把这种函数称为：**激活函数（activation function）**。从数学角度来说我们有很多`activation function`可以达到这种效果，比如下面这种：\n",
    "\n",
    "从图中可以看到，我们的输入信号不断增加，而输出一直为0，直到我们的输入信号达到阈值，输出立即就产生了，我把这个函数称之为**步进函数(step function)** 步进函数非常突兀，也不太符合现实，我们在神经网络中使用的是更加平滑的激活函数，我们通常采用这种函数是**sigmoid 函数**，相比于步进函数的Z字形，sigmoid更像是一个S形。\n",
    "\n",
    "虽然在神经网络的领域，我们有许多其它的函数也有类似的效果，但是sigmoid函数的有几个优点：\n",
    "\n",
    "1. 输出范围有限，数据在传递过程中不容易发散\n",
    "2. 足够简单，求导也很方便\n",
    "\n",
    "sigmoid函数的式子可以写成以下形式：$y = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "这是一个很简单的方程，关于这个公式的数学含义在这里也不多做介绍。从这个公式的形式来看也可以发现它足够简单，容易计算，在接下来的部分中我们就会了解计算简单这件事到底是多么重要了。\n",
    "\n",
    "现在让我们回到神经网络，开始考虑下如何建立一个神经网络的模型。\n",
    "\n",
    "首先需要认识到的一件事，在生物神经网络中，我们并不是只有一个输入信号源，而是会有多个，因此在我们的神经网络中也会有类似的处理，我们处理的不是一个输入的数据而是需要同时处理许多输入的数据。但是我们应该如何处理这些输入的数据呢？\n",
    "\n",
    "其实也很简单，我们把它们都加起来，然后代入sigmoid函数之中，就可以控制输出了。\n",
    "\n",
    "如果那些经过计算的信号达到了阈值，则触发输出信号的操作。不过需要注意的是，每一个神经元都会接收多个输入，同样的也会输出多个值。\n",
    "我们怎么样在人工神经网络中实现类似的模型呢？\n",
    "一个通用的方法就是引入**层（layers）**的概念，我们把一定数量的神经元放入一个层中，每一个神经元都会连接到它上一层的所有神经元以及下一层的所有神经元。\n",
    "\n",
    "![neron](img/neuron1.png)\n",
    "\n",
    "在图中我们能看到一个三层的人工神经网络，每一层都有三个神经元，我们更通常的叫法是：**节点（node）**。\n",
    "\n",
    "这么一看我们的人工神经网络是不是有点像生物的神经网络了。但是，我们怎么用这个神经网络去学习呢？我们怎么做才能得到最终的输出值？以及是否有参数能够供我们像之前的分类器那样做调整？\n",
    "\n",
    "在下面的图中我们可以看到，神经元直接的连接依旧，但是连接之间多了一个权重的属性，如果权重比较低，那么我们就会抑制输入，如果权重比较高，我们会增强输入。\n",
    "\n",
    "![neron2](img/neuron2.png)\n",
    "\n",
    "对于神经网络来说，权重是一个非常重要的概念，值得我们着重介绍。\n",
    "\n",
    "在这个神经网络中，权重$w_{1,2}$代表某一层的节点1到另一层的节点2的权重值。\n",
    "\n",
    "当然我们可能会有疑问，为什么两层之间的所有节点都要相互连接，实际上我们确实不需要全连接，但是对于计算机来说，这种每一层都保持一致性的形式更容易处理。而且全连接也并没有确切的坏处，我们在运行神经网络的时候其实是会将并不重要的输入信息消解掉。\n",
    "\n",
    "这是什么意思呢？这意味着神经网络在通过训练数据不断校正的过程中，有些权重值其实是会变成0或者无限接近于0的。而权重值为0的连接也意味着输入值其实不对输出产生影响了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 神经网络是怎么计算的\n",
    "\n",
    "上一节中我们看到了一个全连接的神经网络，接下来我们要看如何利用神经网络的结构进行计算。当然，整个计算过程其实有点复杂，为了简化计算过程，我们会简化上一节中的神经网络，使用一个两层，每一层只有两个节点的神经网络进行计算。\n",
    "\n",
    "![neuron3](img/neuron3.png)\n",
    "\n",
    "我们假设输入的值分别为1.0和0.5，每个节点都会把输入值加总并代入激活函数$y = \\frac{1}{1 + e^{-x}}$。其中x是输入的加总，y是这个神经元的输出值。\n",
    "\n",
    "![neuron4](img/neuron4.png)\n",
    "\n",
    "我们之前说过，权重是神经网络中非常重要的概念，那么我们应该如何定义权重呢？\n",
    "\n",
    "其实我们可以随便取几个数作为权重，比如\n",
    "\n",
    "* $w_{1,1} = 0.9$\n",
    "* $w_{1,2} = 0.2$\n",
    "* $w_{2,1} = 0.3$\n",
    "* $w_{2,2} = 0.8$\n",
    "\n",
    "类似我们在分类器的例子中也是取了一个随机数一样，这个随机数会随着我们不断学习的过程中不断的更新，分类器如此，神经网络也是如此。\n",
    "\n",
    "在我们这个例子的神经网络中，只有四个权重，如果我们在图上把这些权重都标出来，就会是下面这样子：\n",
    "\n",
    "![neuron5](img/neuron5.png)\n",
    "\n",
    "让我们开始计算吧。\n",
    "\n",
    "第一层是用来接收输入的，我们称其为输入层，在这一层中我们并不需要对输入做其它计算。欢呼吧，我们处理了神经网络的第一层。接下来让我们开始处理第二层吧。这一层中我们不得不进行一些计算了，这一层中的每一个节点都需要计算根据每一个权重计算再加总后的输入值，再代入我们的激活函数：$y = \\frac{1}{1 + e^{-x}}$。\n",
    "\n",
    "首先不同连接的输入值乘上其对应的权重，接下来把计算后的输入值加总后代入激活函数，就成为了该节点的输出值了。\n",
    "我们可以据此来计算第二层第一个节点的输出值$x = 1.0*0.9 + 0.5 * 0.3 = 1.05$。\n",
    "\n",
    "如果没有增加权重这个概念的话，那么我们的输入值就会成为$1.0+0.5 = 1.5$正是权重这个概念的引入才能使得我们的原始数据中的噪声不会太影响运行结果。\n",
    "代入我们之前所定义的激活函数之后，就能得到节点的输出值。\n",
    "\n",
    "接下来我们处理剩下的节点，也是相同的流程，我们将其推而广之，从2层的神经网络到8层，10层，100层都是一样的，我们的节点从2个，8个10个100个也都是一样。\n",
    "\n",
    "好了，我们现在需要处理的是一个两层两个节点的神经网络，计算起来还是比较简单的，但如果我们现在有一个10层的神经网络，每一层有1000个节点，我的计算就要计算$(10*1000 + 10 * 1000 + 10*1000 + 10 * 1000 +10*1000 + 10 * 1000 + 10*1000 + 10 * 1000 + 10*1000 + 10 * 1000) = 100000$，是的，每运行一次神经网络都要计算10w次，那么我的计算机应该如果进行运算呢？\n",
    "\n",
    "答案是：线性代数。关于线性代数的基础知识请移步[这里](../Part_1_Foundation/Linear Algebra.ipynb)，在本篇中不做详述，假设我们已经对线性代数有基本的了解，那么线性代数如何与神经网络相关联呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 矩阵的魔力\n",
    "\n",
    "回到我们神经网络的图上来，我们的目标是每个让输入值都乘上相应的权重才是对于神经元有效的输入，那么这个计算过程，我们可以用下面这个矩阵来表示：\n",
    "\n",
    "$$ \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   w_{1,1} & w_{2,1} \\\\\n",
    "   w_{1,2} & w_{2,2}\n",
    "  \\end{matrix}\n",
    "  \\right] \n",
    "   \\left[\n",
    " \\begin{matrix}\n",
    "   input_1 \\\\\n",
    "   input_2 \n",
    "  \\end{matrix}\n",
    "  \\right] \n",
    "  =\n",
    "    \\left[\n",
    "   \\begin{matrix}\n",
    "   (input_1 * w_{1,1} + input_2 * w_{2,1}) \\\\\n",
    "   (input_1 * w_{1,2} + input_2 * w_{2,2}) \n",
    "  \\end{matrix}\n",
    "  \\right] \n",
    "$$ \n",
    "\n",
    "这么一看是不是很神奇？\n",
    "\n",
    "第一个矩阵包含了两个层之间的权重，第二个矩阵包含了从输入层接收到的输入信号，这个两个矩阵相乘的结果是我们得到了修正过的输入值。需要注意的是，我们首先要将这个矩阵计算完之后，才可以加上sigmod函数的计算。\n",
    "\n",
    "好了，现在我们用一个公式来总结一下，对于我们神经网络的第二层，输入值可以用以下公式表示：\n",
    "\n",
    "$$X = W · I$$\n",
    "\n",
    "其中$W$代表权重值矩阵，$I$代表输入值矩阵，$X$就是经过计算后的每个神经元应该接收的值。让我们想想这件事的神奇之处：我们其实用了一步的运算就可以知道下一层应该接收到的信号值，无论它有2个节点还是200个节点，我们不必要为每个节点运算一遍，而是将其放入一个矩阵之中统一进行了运算。这就是线性代数对于神经网络的意义了！\n",
    "\n",
    "那么我们又如何来处理激活函数呢？我们可以将其放入公式之中：\n",
    "\n",
    "$$O = sigmoid(X)$$\n",
    "\n",
    "$X$是我们的节点接收到的信号值，代入激活函数，得到了我们最后输出的矩阵$O$。\n",
    "\n",
    "总结一下：\n",
    "\n",
    "1. 在神经网络中的很多计算我们都可以将其用矩阵（确切的说是张量）的形式来表达\n",
    "2. 对于一些编程语言来说（比如Python）是能够理解张量这种形式的，对于我们来说这也意味着它们运行矩阵计算的时候效率非常高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 计算一个三层的神经网络\n",
    "\n",
    "在之前我们已经介绍了很多理论性的东西，我们知道神经网络的构成，也知道了可以使用矩阵来计算这个神经网络，在这一节中，我们将会以一个三层，每一层三个节点的神经网络作为例子，详述其计算过程。\n",
    "\n",
    "为什么用一个三层结构的神经网络的原因很好解释，就算将这个神经网络的结构推广至30层，每一层有100个节点，其计算过程也不会有太大的差别。\n",
    "\n",
    "下图是我们这个三层神经网络的例子，由于篇幅的关系我没有将节点之间连接权重都标识出来\n",
    "\n",
    "![neuron6](img/neuron6.png)\n",
    "\n",
    "我们先介绍一下这个神经网络，我们看到的第一层，称之为**输入层（Input layer）**，第三层，也就是最后一层，我们称之为**输出层（Output layer）**，中间的那一层，当然也可能会是很多层，我们称之为**隐藏层（hidden layer）**。\n",
    "\n",
    "隐藏层听起来很神秘，但我们称其为隐藏层的原因只是因为对于最后的输出层来说，隐藏层中的输出基本是不可见的，也不是输出层最为关心的。\n",
    "\n",
    "在这个图中我们的可以看到$w_{1,1}=0.9$，$w_{1,2}=0.2$，我们可以用一个矩阵把layer1与layer2之间节点的连接关系表示出来：\n",
    "\n",
    "$$ \n",
    " \\left[\n",
    " \\begin{matrix}\n",
    "   0.9 & 0.3 & 0.5 \\\\\n",
    "   0.2 & 0.4 & 0.8 \\\\\n",
    "   0.5 & 0.6 & 0.7\n",
    "  \\end{matrix}\n",
    "  \\right] \n",
    "$$ \n",
    "\n",
    "输入层的处理很简单，因为输入层的作用只有一个：接收输入。\n",
    "\n",
    "现在我们处理隐藏层的数据。记得我们的操作，首先要处理所有节点的输入值，乘上权重，接下来再代入激活函数。上一节中已经说过了用矩阵的方式来表达计算：$X = W · I$\n",
    "\n",
    "我们可以从矩阵中看到权重值$w_{1,1}$为0.9，就像是在我们的神经网络图中展现出的一样，同样的我们可以看到输入层的第二个节点与隐藏层的第二个节点的权重值为0.8。\n",
    "\n",
    "好了，相信都能理解到这个矩阵的妙处了，我们可以将矩阵中输入层到隐藏层的权重描述为 $w_{input_hidden}$, 而隐藏层到输出层的权重值矩阵我们用$w_{hidden_output}$来表示。\n",
    "\n",
    "现在我们已经定义了所有层之间的权重值，接下来需要做的就是将已经权重值与输入层联系起来，为了方便区分，可以将其称为 $w_{hidden_input_hidden}$ I 计算过程如下所示：\n",
    "\n",
    "在这里不再详细介绍计算的过程，而且最终我们也是通过写代码的方式让计算机来做矩阵计算的工作，这部分的代码可以在实例部分找到。\n",
    "到现在为止我们已经将输入层的值经过调整后传输到隐藏层了，但是接下来我们还有工作需要做，就是将其放入激活函数中。\n",
    "\n",
    "在代入激活函数之后，我们会发现最终的值都在0和1之间，如果看之前那个激活函数的图就可以发现这个事实了，在图中y坐标的取值在0与1之间，这就意味着当输入值x无限大的时候，我们的输出值也只是趋向于1，这样的特性是能够帮助我们去除数据中异常对于我们模型的影响。\n",
    "\n",
    "如果是一个2层的神经网络，我们的计算就算完成了，但是在这个3层的例子中我们还需要再计算一层，这个第二层与第三层的计算并没有什么不同，其实无论是第三层还是第五十三层的计算都是一样的。所以我们层与层之间计算都是相同的，隐藏层与输出层可以用公式\n",
    "\n",
    "$X_{output} =W_{hidden_output} · O_{output}$\n",
    "\n",
    "经过计算之后我们的输入经过输入层，隐藏层的计算之后在输出层得到了最终的结果。\n",
    "\n",
    "我们的神经网络计算就是这样，接下来呢？\n",
    "\n",
    "想一想我们最开始的例子，我们运算出了结果，接着将其与训练数据进行对比，根据误差来调整参数，那么对于我们的神经网络，其实也是相同的处理流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 更新权重\n",
    "\n",
    "在之前分类器的例子中我们根据输出值产生的误差更新参数，在这神经网络的训练中其实也是遵循这样的规则。\n",
    "但有一个问题，神经网络中每个节点都有多个连接，我们将输出层的值与实际值对比之后就能得到误差了，但是怎么更新到每一层的权重值呢？像图中所示\n",
    "\n",
    "如果只有一个节点的话更新权重值就不是一件很复杂的事了，如果有两个节点的话，我们应该如何做呢？从直觉上来考虑这件事，用全量的误差来更新一个权重值肯定不太合适，毕竟每个权重值都贡献了输出值。\n",
    "\n",
    "有一个方法是等量的分割误差，比如有两个节点误差值就除以2，就像图中所示\n",
    "\n",
    "虽然我们在神经网络中没有这么做过，但是如果真的采用这种直接的方法效果应该也不会太差。\n",
    "\n",
    "另一个思路根据权重值来分割误差，权重值越大给予的误差也就越多，这样其实很容易理解，权重值越大，对误差应该负的责任也越大\n",
    "\n",
    "比如下图所示，权重值分别为3.0和1.0，那么$w_{1,1}$需要处理四分之三的误差，$w_{2,1}$需要处理四分之一的误差。\n",
    "\n",
    "我们可以将这个思路推而广之，对于100节点的例子来说，我们同样可以将误差按比例分配给不同的连接。\n",
    "\n",
    "可以看出，权重的用处主要有二，一是用来影响输入信号从而控制输出，二是利用权重来反向分配误差，从而校正神经网络模型。\n",
    "\n",
    "在此之前，让我们先定义一些概念。\n",
    "\n",
    "1. 输出层的误差表示为$e_{output}$\n",
    "2. 隐藏层的误差表示为$e_{hidden}$\n",
    "3. 将隐藏层与输出层之间的权重表示为$W_{ho}$\n",
    "4. 输入层与隐藏层之间的权重表示为$W_{ih}$\n",
    "\n",
    "$e_{output}$的一个标准传播方式是从输出层根据$W_{ho}$得到$e_{hidden}$，再根据$W_{ih}$传递到输入层。即使有再多的层，我们也是不断的重复这个过程，所以我们回想一下神经网络的计算方式，是从输入层到输出层的，而误差的计算方式是从输出层到输入层的，正好是一正一反，像误差的这种传播方式我们就称之为**反向传播(backpropagation)**。\n",
    "\n",
    "现在我们知道了误差怎样从输出层传播到输入层，但是问题又来了，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
