{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Word2Vec做自然语言分析\n",
    "\n",
    "## 准备语料\n",
    "\n",
    "网络上高质量的语料比较少，中文的高质量语料就更少了，我在知乎的专栏上找到一篇文章总结了[各个领域的公开数据集](https://zhuanlan.zhihu.com/p/25138563)。\n",
    "\n",
    "在本文中我们使用的是[搜狗新闻语料数据](https://www.sogou.com/labs/resource/list_news.php)，\n",
    "\n",
    "我下载的新闻语料压缩包大小在640MB，解压后是一个dat格式的文件，数据格式为：\n",
    "\n",
    "```xml\n",
    "<doc>\n",
    "<url>页面URL</url>\n",
    "<docno>页面ID</docno>\n",
    "<contenttitle>页面标题</contenttitle>\n",
    "<content>页面内容</content>\n",
    "</doc>\n",
    "```\n",
    "\n",
    "原始的语料数据是GBK格式的，在linux上是乱码，需要将其转换为UTF-8编码，顺带的我们也只需要`<content>`标签内的内容，因此可以一并将其处理了。使用shell命令可以一步完成：\n",
    "\n",
    "```shell\n",
    "cat news_tensite_xml.dat | iconv -f gbk -t utf-8 -c | grep \"<content>\"  > sougou_corpus.txt \n",
    "```\n",
    "\n",
    "执行命令后效果如图：\n",
    "\n",
    "![sourgou_corpus.txt](img/sougou_corpus.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词\n",
    "\n",
    "准备好语料之后，接下来需要做的是分词了。这一步骤对于中文很重要，因为英文是由一个个单词组成，而中文是由字组成，很多时间我们需要由字组成词之后才有意义。\n",
    "\n",
    "中文的NLP处理中有一些用来做分词的工具，在这里我们使用[结巴分词](https://github.com/fxsjy/jieba)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*- \n",
    "import logging\n",
    "import os.path\n",
    "import jieba\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用正则表达式提取content\n",
    "def reTest(content):\n",
    "    reContent = re.sub('<content>|</content>', '', content)\n",
    "    return reContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化日志配置\n",
    "program = os.path.basename(sys.argv[0])\n",
    "logger = logging.getLogger(program)\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "# logging.info(\"running %s\" % ' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-04-17 22:17:30,288: DEBUG: Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2018-04-17 22:17:30,291: DEBUG: Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.615 seconds.\n",
      "2018-04-17 22:17:31,905: DEBUG: Loading model cost 1.615 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-04-17 22:17:31,907: DEBUG: Prefix dict has been built succesfully.\n",
      "2018-04-17 22:18:08,713: INFO: Saved 10000\n",
      "2018-04-17 22:18:48,057: INFO: Saved 20000\n",
      "2018-04-17 22:19:24,854: INFO: Saved 30000\n",
      "2018-04-17 22:20:03,984: INFO: Saved 40000\n",
      "2018-04-17 22:20:41,316: INFO: Saved 50000\n",
      "2018-04-17 22:21:18,775: INFO: Saved 60000\n",
      "2018-04-17 22:21:55,472: INFO: Saved 70000\n",
      "2018-04-17 22:22:30,392: INFO: Saved 80000\n",
      "2018-04-17 22:23:05,705: INFO: Saved 90000\n",
      "2018-04-17 22:23:45,710: INFO: Saved 100000\n",
      "2018-04-17 22:24:25,828: INFO: Saved 110000\n",
      "2018-04-17 22:25:07,726: INFO: Saved 120000\n",
      "2018-04-17 22:25:47,968: INFO: Saved 130000\n",
      "2018-04-17 22:26:29,907: INFO: Saved 140000\n",
      "2018-04-17 22:27:12,698: INFO: Saved 150000\n",
      "2018-04-17 22:27:54,209: INFO: Saved 160000\n",
      "2018-04-17 22:28:34,910: INFO: Saved 170000\n",
      "2018-04-17 22:29:15,336: INFO: Saved 180000\n",
      "2018-04-17 22:29:54,650: INFO: Saved 190000\n",
      "2018-04-17 22:30:34,484: INFO: Saved 200000\n",
      "2018-04-17 22:31:15,049: INFO: Saved 210000\n",
      "2018-04-17 22:31:55,435: INFO: Saved 220000\n",
      "2018-04-17 22:32:34,105: INFO: Saved 230000\n",
      "2018-04-17 22:33:12,822: INFO: Saved 240000\n",
      "2018-04-17 22:33:51,352: INFO: Saved 250000\n",
      "2018-04-17 22:34:27,371: INFO: Saved 260000\n",
      "2018-04-17 22:35:06,093: INFO: Saved 270000\n",
      "2018-04-17 22:35:43,711: INFO: Saved 280000\n",
      "2018-04-17 22:36:21,746: INFO: Saved 290000\n",
      "2018-04-17 22:36:59,410: INFO: Saved 300000\n",
      "2018-04-17 22:37:34,664: INFO: Saved 310000\n",
      "2018-04-17 22:38:11,751: INFO: Saved 320000\n",
      "2018-04-17 22:38:49,246: INFO: Saved 330000\n",
      "2018-04-17 22:39:26,403: INFO: Saved 340000\n",
      "2018-04-17 22:40:03,700: INFO: Saved 350000\n",
      "2018-04-17 22:40:40,999: INFO: Saved 360000\n",
      "2018-04-17 22:41:17,191: INFO: Saved 370000\n",
      "2018-04-17 22:41:54,165: INFO: Saved 380000\n",
      "2018-04-17 22:42:32,474: INFO: Saved 390000\n",
      "2018-04-17 22:43:08,754: INFO: Saved 400000\n",
      "2018-04-17 22:43:47,173: INFO: Saved 410000\n",
      "2018-04-17 22:44:26,135: INFO: Saved 420000\n",
      "2018-04-17 22:45:02,591: INFO: Saved 430000\n",
      "2018-04-17 22:45:41,175: INFO: Saved 440000\n",
      "2018-04-17 22:46:16,749: INFO: Saved 450000\n",
      "2018-04-17 22:46:55,549: INFO: Saved 460000\n",
      "2018-04-17 22:47:33,261: INFO: Saved 470000\n",
      "2018-04-17 22:48:10,164: INFO: Saved 480000\n",
      "2018-04-17 22:48:48,567: INFO: Saved 490000\n",
      "2018-04-17 22:49:21,545: INFO: Saved 500000\n",
      "2018-04-17 22:49:59,067: INFO: Saved 510000\n",
      "2018-04-17 22:50:33,367: INFO: Saved 520000\n",
      "2018-04-17 22:51:10,084: INFO: Saved 530000\n",
      "2018-04-17 22:51:44,274: INFO: Saved 540000\n",
      "2018-04-17 22:52:17,848: INFO: Saved 550000\n",
      "2018-04-17 22:52:51,885: INFO: Saved 560000\n",
      "2018-04-17 22:53:31,201: INFO: Saved 570000\n",
      "2018-04-17 22:54:04,772: INFO: Saved 580000\n",
      "2018-04-17 22:54:43,569: INFO: Saved 590000\n",
      "2018-04-17 22:55:17,028: INFO: Saved 600000\n",
      "2018-04-17 22:55:53,867: INFO: Saved 610000\n",
      "2018-04-17 22:56:28,333: INFO: Saved 620000\n",
      "2018-04-17 22:56:58,886: INFO: Saved 630000\n",
      "2018-04-17 22:57:32,861: INFO: Saved 640000\n",
      "2018-04-17 22:58:06,729: INFO: Saved 650000\n",
      "2018-04-17 22:58:33,672: INFO: Saved 660000\n",
      "2018-04-17 22:59:05,492: INFO: Saved 670000\n",
      "2018-04-17 22:59:31,334: INFO: Saved 680000\n",
      "2018-04-17 23:00:03,501: INFO: Saved 690000\n",
      "2018-04-17 23:00:36,681: INFO: Saved 700000\n",
      "2018-04-17 23:01:08,278: INFO: Saved 710000\n",
      "2018-04-17 23:01:41,546: INFO: Saved 720000\n",
      "2018-04-17 23:02:17,604: INFO: Saved 730000\n",
      "2018-04-17 23:02:51,666: INFO: Saved 740000\n",
      "2018-04-17 23:03:22,339: INFO: Saved 750000\n",
      "2018-04-17 23:04:01,039: INFO: Saved 760000\n",
      "2018-04-17 23:04:39,786: INFO: Saved 770000\n",
      "2018-04-17 23:05:15,545: INFO: Saved 780000\n",
      "2018-04-17 23:05:50,629: INFO: Saved 790000\n",
      "2018-04-17 23:06:20,726: INFO: Saved 800000\n",
      "2018-04-17 23:06:46,228: INFO: Saved 810000\n",
      "2018-04-17 23:07:12,819: INFO: Saved 820000\n",
      "2018-04-17 23:07:42,524: INFO: Saved 830000\n",
      "2018-04-17 23:08:15,979: INFO: Saved 840000\n",
      "2018-04-17 23:08:36,180: INFO: Saved 850000\n",
      "2018-04-17 23:09:07,066: INFO: Saved 860000\n",
      "2018-04-17 23:09:37,840: INFO: Saved 870000\n",
      "2018-04-17 23:10:06,251: INFO: Saved 880000\n",
      "2018-04-17 23:10:34,746: INFO: Saved 890000\n",
      "2018-04-17 23:10:59,257: INFO: Saved 900000\n",
      "2018-04-17 23:11:28,404: INFO: Saved 910000\n",
      "2018-04-17 23:11:57,717: INFO: Saved 920000\n",
      "2018-04-17 23:12:27,411: INFO: Saved 930000\n",
      "2018-04-17 23:12:57,113: INFO: Saved 940000\n",
      "2018-04-17 23:13:29,717: INFO: Saved 950000\n",
      "2018-04-17 23:14:10,948: INFO: Saved 960000\n",
      "2018-04-17 23:14:47,548: INFO: Saved 970000\n",
      "2018-04-17 23:15:25,609: INFO: Saved 980000\n",
      "2018-04-17 23:16:04,018: INFO: Saved 990000\n",
      "2018-04-17 23:16:40,762: INFO: Saved 1000000\n",
      "2018-04-17 23:17:21,379: INFO: Saved 1010000\n",
      "2018-04-17 23:17:55,409: INFO: Saved 1020000\n",
      "2018-04-17 23:18:29,409: INFO: Saved 1030000\n",
      "2018-04-17 23:19:07,869: INFO: Saved 1040000\n",
      "2018-04-17 23:19:42,724: INFO: Saved 1050000\n",
      "2018-04-17 23:20:20,540: INFO: Saved 1060000\n",
      "2018-04-17 23:20:53,691: INFO: Saved 1070000\n",
      "2018-04-17 23:21:21,095: INFO: Saved 1080000\n",
      "2018-04-17 23:21:54,502: INFO: Saved 1090000\n",
      "2018-04-17 23:22:28,255: INFO: Saved 1100000\n",
      "2018-04-17 23:23:00,726: INFO: Saved 1110000\n",
      "2018-04-17 23:23:36,679: INFO: Saved 1120000\n",
      "2018-04-17 23:24:07,277: INFO: Saved 1130000\n",
      "2018-04-17 23:24:41,522: INFO: Saved 1140000\n",
      "2018-04-17 23:25:12,400: INFO: Saved 1150000\n",
      "2018-04-17 23:25:43,955: INFO: Saved 1160000\n",
      "2018-04-17 23:26:16,650: INFO: Saved 1170000\n",
      "2018-04-17 23:26:50,078: INFO: Saved 1180000\n",
      "2018-04-17 23:27:23,475: INFO: Saved 1190000\n",
      "2018-04-17 23:27:58,756: INFO: Saved 1200000\n",
      "2018-04-17 23:28:35,883: INFO: Saved 1210000\n",
      "2018-04-17 23:29:11,702: INFO: Saved 1220000\n",
      "2018-04-17 23:29:48,512: INFO: Saved 1230000\n",
      "2018-04-17 23:30:28,023: INFO: Saved 1240000\n",
      "2018-04-17 23:31:02,046: INFO: Saved 1250000\n",
      "2018-04-17 23:31:42,289: INFO: Saved 1260000\n",
      "2018-04-17 23:32:18,573: INFO: Saved 1270000\n",
      "2018-04-17 23:32:59,713: INFO: Saved 1280000\n",
      "2018-04-17 23:33:33,400: INFO: Saved 1290000\n",
      "2018-04-17 23:34:13,971: INFO: Saved 1300000\n",
      "2018-04-17 23:34:49,857: INFO: Saved 1310000\n",
      "2018-04-17 23:35:33,089: INFO: Saved 1320000\n",
      "2018-04-17 23:36:06,689: INFO: Saved 1330000\n",
      "2018-04-17 23:36:48,730: INFO: Saved 1340000\n",
      "2018-04-17 23:37:21,229: INFO: Saved 1350000\n",
      "2018-04-17 23:37:59,532: INFO: Saved 1360000\n",
      "2018-04-17 23:38:33,611: INFO: Saved 1370000\n",
      "2018-04-17 23:39:10,761: INFO: Saved 1380000\n",
      "2018-04-17 23:39:45,245: INFO: Saved 1390000\n",
      "2018-04-17 23:40:18,455: INFO: Saved 1400000\n",
      "2018-04-17 23:40:55,361: INFO: Saved 1410000\n",
      "2018-04-17 23:41:03,528: INFO: Finished Saved 1411996\n"
     ]
    }
   ],
   "source": [
    "# 获取当前文件所在绝对路径\n",
    "base_path = os.path.abspath('.')\n",
    "dataset_path = base_path + '/' + 'dataset'\n",
    "input_corpus = dataset_path + '/' + 'sougou_corpus.txt'\n",
    "output_corpus = dataset_path + '/' + 'seg_sougou_corpus.txt'\n",
    "\n",
    "i = 0\n",
    "try:\n",
    "    fo = open(output_corpus, 'w')\n",
    "    with open(input_corpus, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            new_sentence = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', reTest(line))\n",
    "#            line_seg = jieba.cut(reTest(line))\n",
    "            line_seg = jieba.cut(new_sentence)\n",
    "            fo.write(' '.join(line_seg)) # \n",
    "            i = i + 1\n",
    "            if (i % 10000 == 0):\n",
    "                logger.info(\"Saved \" + str(i))\n",
    "finally:\n",
    "    fo.close()\n",
    "        \n",
    "logger.info(\"Finished Saved \" + str(i))\n",
    "# finput = open(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath('.')\n",
    "dataset_path = base_path + '/' + 'dataset'\n",
    "output_corpus = dataset_path + '/' + 'seg_sougou_corpus.txt'\n",
    "\n",
    "words_list = []\n",
    "with open(output_corpus, 'r') as f:\n",
    "    for line in f:\n",
    "        words_list = line.split()\n",
    "    \n",
    "#word_counts = Counter(words_list) \n",
    "#trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "logger.info(words_list[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
